# Ollama

Welcome to the Ollama project! This is a FastAPI-based backend designed for handling various AI-powered workflows, including resume summarization, image description, and question-answering using large language models (LLMs). The project also includes a Streamlit UI for seamless interaction, making it easier to use and visualize the outputs. The LLM model used in this system is Llava-LLama3, which powers the advanced capabilities of the platform for natural language processing and understanding.



## 📁 Project Structure
🧩 Components
🏗️ Backend
FastAPI: Handles API endpoints for various AI-powered services.
   Services:
      resume_summarizer.py: Handles resume summarization.
      image_describer.py: Generates descriptions for uploaded images.
      qa.py: Provides question-answering capabilities.
   Routes:
      resume_summarizer_endpoint.py
      image_describer_endpoint.py
      qa_endpoint.py
   Utilities:
      pdf_processor: Extracts text from PDF files.
   Schemas:
      output_schema.py: Defines the data models for API responses.
🎨 Frontend
Streamlit: Provides a user-friendly interface for interacting with the backend services.
   streamlit_app.py: The main application script for the frontend UI.
🐳 Docker
Dockerfiles: Containerize both backend and frontend applications for easy deployment.


## 🚀 Features

1. **Resume Summarization:**
   - Upload a resume in PDF format.
   - Extract text and summarize its contents using a large language model.

2. **Image Description:**
   - Upload an image and receive a detailed description generated by an AI model.

3. **Question Answering:**
   - Ask any question, and the AI provides accurate responses.

4. **Utilities:**
   - PDF text extraction functionality to support workflows.

---

## 🛠️ Setup & Installation

### Prerequisites

- Virtual environment (recommended)
- FastAPI, Uvicorn, and other dependencies (see `requirements.txt`)

### Installation Steps

1. **Clone the Repository:**
   ```bash
   git clone https://github.com/mahsa_projects/ollama.git
      touch .env
      cd Backend
         python -m venv .venv
         source .venv/Scripts/activate
         pip install -r requirements.txt
         uvicorn app.main:app --reload

      cd Frontend
      streamlit run streamlit_app.py


      
   



   


